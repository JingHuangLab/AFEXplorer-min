Script started on Thu 07 Dec 2023 06:10:38 PM CST
ksongzl@zimei:~/projects/2.afexplorer\(base) [songzl@node38 2.afexplorer]$ conda activate afex
ksongzl@zimei:~/projects/2.afexplorer\(afex) [songzl@node38 2.afexplorer]$ bash 1_run_afex.sh 
/home/songzl/software/miniconda3/envs/afex/lib/python3.9/site-packages/absl/flags/_validators.py:233: UserWarning: Flag --nsteps has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!
  mark_flag_as_required(flag_name, flag_values)
/home/songzl/software/miniconda3/envs/afex/lib/python3.9/site-packages/absl/flags/_validators.py:233: UserWarning: Flag --nclust has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!
  mark_flag_as_required(flag_name, flag_values)
I1207 18:10:53.016483 46971848814272 xla_bridge.py:353] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I1207 18:10:55.844819 46971848814272 xla_bridge.py:353] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I1207 18:10:55.845650 46971848814272 xla_bridge.py:353] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1207 18:10:55.845746 46971848814272 xla_bridge.py:353] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
AFEX started.
Runing step 0
2023-12-07 18:11:27.870877: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:

  reduce.637 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2023-12-07 18:11:29.773473: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 2.902703575s
Constant folding an instruction is taking > 1s:

  reduce.637 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2023-12-07 18:11:31.819147: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 2s:

  reduce.50 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2023-12-07 18:11:32.828583: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 3.009495768s
Constant folding an instruction is taking > 2s:

  reduce.50 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2023-12-07 18:14:42.502874: W external/org_tensorflow/tensorflow/tsl/framework/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 34.94GiB (rounded to 37516437504)requested by op 
2023-12-07 18:14:42.510035: W external/org_tensorflow/tensorflow/tsl/framework/bfc_allocator.cc:492] *************************************************___________________________________________________
2023-12-07 18:14:42.535467: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2153] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 37516437464 bytes.
BufferAssignment OOM Debugging.
BufferAssignment stats:
             parameter allocation:   18.54GiB
              constant allocation:       193B
        maybe_live_out allocation:  131.51MiB
     preallocated temp allocation:   34.94GiB
                 total allocation:   53.61GiB
Peak buffers:
	Buffer 1:
		Size: 11.13GiB
		Entry Parameter Subshape: f32[48,512,475,256]
		==========================

	Buffer 2:
		Size: 5.16GiB
		Operator: op_name="jit(apply_fn)/jit(main)/alphafold/alphafold_iteration/evoformer/__layer_stack_no_state_1/while[cond_nconsts=0 body_nconsts=104]" source_file="/home/songzl/projects/2.afexplorer/afexplore/alphafold/model/layer_stack.py" source_line=156
		XLA Label: fusion
		Shape: f32[48,475,475,128]
		==========================

	Buffer 3:
		Size: 5.16GiB
		Entry Parameter Subshape: f32[48,475,475,128]
		==========================

	Buffer 4:
		Size: 3.69GiB
		XLA Label: copy
		Shape: f32[118,4,8,512,512]
		==========================

	Buffer 5:
		Size: 3.69GiB
		Operator: op_name="jit(apply_fn)/jit(main)/alphafold/alphafold_iteration/evoformer/__layer_stack_no_state_1/while/body/remat/evoformer_iteration/msa_column_attention/broadcast_in_dim[shape=(118, 4, 8, 512, 512) broadcast_dimensions=()]" source_file="/home/songzl/projects/2.afexplorer/afexplore/alphafold/model/mapping.py" source_line=187
		XLA Label: broadcast
		Shape: f32[118,4,8,512,512]
		==========================

	Buffer 6:
		Size: 237.50MiB
		Operator: op_name="jit(apply_fn)/jit(main)/alphafold/alphafold_iteration/evoformer/__layer_stack_no_state_1/while/body/remat/evoformer_iteration/msa_column_attention/while[cond_nconsts=0 body_nconsts=19]" source_file="/home/songzl/projects/2.afexplorer/afexplore/alphafold/model/mapping.py" source_line=187
		XLA Label: fusion
		Shape: f32[475,512,256]
		==========================

	Buffer 7:
		Size: 237.50MiB
		XLA Label: copy
		Shape: f32[475,512,256]
		==========================

	Buffer 8:
		Size: 237.50MiB
		Operator: op_name="jit(apply_fn)/jit(main)/alphafold/alphafold_iteration/evoformer/__layer_stack_no_state_1/while/body/remat/evoformer_iteration/msa_column_attention/broadcast_in_dim[shape=(475, 512, 256) broadcast_dimensions=()]" source_file="/home/songzl/projects/2.afexplorer/afexplore/alphafold/model/mapping.py" source_line=182
		XLA Label: broadcast
		Shape: f32[475,512,256]
		==========================

	Buffer 9:
		Size: 237.50MiB
		XLA Label: fusion
		Shape: f32[512,475,256]
		==========================

	Buffer 10:
		Size: 237.50MiB
		Operator: op_name="jit(apply_fn)/jit(main)/alphafold/alphafold_iteration/evoformer/broadcast_in_dim[shape=(512, 475, 256) broadcast_dimensions=()]" source_file="/home/songzl/projects/2.afexplorer/afexplore/alphafold/model/modules.py" source_line=1997
		XLA Label: broadcast
		Shape: f32[512,475,256]
		==========================

	Buffer 11:
		Size: 237.50MiB
		Operator: op_name="jit(apply_fn)/jit(main)/alphafold/alphafold_iteration/evoforme